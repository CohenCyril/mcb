\chapter{More statements, more proofs}{Advanced specifications}

in addition to the photos:
\begin{itemize}
\item talk about /= in 2 as a decorator for elim on a list (since arithmetics is all loked with nosimpl).
\item talk about nosimpl in 3.3, say that according to our experience simpl is not
always a good idea hence nosimpl.
\item good practice (3.3 or 3.4): state and prove the fixpoint unfolding/folding
equations.
\end{itemize}

\section{Type Theory and the Curry-Howard correspondence}

By now, the informed reader is likely to wonder when the authors of the book
will eventually mention the mathematical foundation of Coq, a variant of 
type theory called \mcbCIC{}, and
in particular explain the Curry-Howard isomorphism.  Indeed this text departs
from the standard presentation of Coq, that typically starts by 
presenting the logic and showing how standard connectives like 
conjunction or disjunction are defined in term of inductive types.

We made a deliberate choice to put forward the programming aspects of
type theory and how programs can be used both to express computable functions
and decidable predicates or boolean connectives.  
Indeed this formalization choice is one of the main ingredients of the \mcbMC{}
library.  The computational behavior of program is at the core of
type theory, i.e. the fact that a function computes to a value requires no
proof.  This fact provided a powerful form of automation, and as we will
see later will also ease the construction of ``sub sets''.

Now we really need to provide a minimal presentation of the statements as types
correspondence.  The universe of boolean predicates is in \mcbMC{} limited: one
cannot for example express a general $\exists$ quantification, and also the
status of the $\forall$ quantification has been so far never really explained.
This presentation is far from being exhaustive, the interested reader can find
a modern presentation of type theory in \cite{hott}.

\subsection{Primitive types and terms formers}

In type theory we say that \emph{logical statements} corresponds to
\emph{types} and that \emph{proofs} correspond to \emph{programs}.

The simplest example one could think of is the formal statement saying that
a proposition $A$ implies itself, namely $A \rightarrow A$.  If one reads
the implication symbol $\to$ as the type of functions, the statements reads
as a function from $A$ to $A$.  A program with that type would be
\C{(fun x : A => x)}.  Such program takes in input a term $x$ of
type $A$, that we here see as a proof of $A$, and returns it, i.e. it
produces in output a proof of $A$.  We say that such program is a
proof of $A \rightarrow A$. 

What is striking here is that the same class of terms represents both programs
and proofs, for example the identity function of natural numbers has the very
same shape of the proof we've just seen.  So we are left with only two
concepts, types and terms, playing a double role.  We now see how types and
terms can be formed.

Regarding types, the primitive type formers we have seen are the function space
$\to$, also logical implication, and the universal quantification
$\forall$, used to describe polymorphic function as well as parametric lemmas.
Standard logical rules describing how one can prove formulas involving these
connectives are:

\begin{center}
\AxiomC{$A$}
\noLine
\UnaryInfC{$\vdots$}
\noLine
\UnaryInfC{$B$}
\RightLabel{$\to_I$}
\UnaryInfC{$A \to B$}
\DisplayProof
\hspace{1cm}
\AxiomC{$B$}
\RightLabel{$\forall_I$ ($x$ fresh)}
\UnaryInfC{$\forall x, B$}
\DisplayProof
\end{center}

The former reads: to prove $A \to B$ one can prove $B$ under the
assumption $A$.  The latter: to prove $\forall x,B$ one can prove $B$
assuming that $x$ is fresh.  We annotate such rules with the terms
corresponding to these proof rules.

\begin{center}
\AxiomC{\C{x : }$~A$}
\noLine
\UnaryInfC{$\vdots$}
\noLine
\UnaryInfC{\C{b : }$~B$}
\RightLabel{$\to_I$}
\UnaryInfC{\C{(fun x : A => b) : }$~A \to B$}
\DisplayProof
\hspace{1cm}
\AxiomC{\C{b : }$B$}
\RightLabel{$\forall_I$}
\UnaryInfC{\C{(fun x : A => b) : }$~\forall x, B$}
\DisplayProof
\end{center}

The anonymous function constructor \C{(fun .. => ..)} serves as a proof
for both rules: the term \C{b} is in both cases a proof of $B$.  The only
difference is that $x$ is only allowed to occur in $B$ in the second rule.
Said otherwise in type theory $\to$ is a special case of $\forall$ where
the bound variable does not occur in the quantified formula.  In other
words there is no semantic difference between \C{(nat -> bool)} and
\C{(forall x : nat, bool)}.

Logical connectives come with elimination rules, in particular

\begin{center}
\AxiomC{$A \to B$}
\AxiomC{$A$}
\RightLabel{$\to_E$}
\BinaryInfC{$B$}
\DisplayProof
\hspace{1cm}
\AxiomC{$\forall x, B$}
\RightLabel{$\forall_E$ ($t$ a term)}
\UnaryInfC{$B[t/x]$}
\DisplayProof
\end{center}

Function application serves as a proof for both rules.

\begin{center}
\AxiomC{\C{f : }$~A \to B$}
\AxiomC{\C{a : }$~A$}
\RightLabel{$\to_E$}
\BinaryInfC{\C{(f a) :}$~B$}
\DisplayProof
\hspace{1cm}
\AxiomC{\C{f : }$~\forall x : A, B$}
\AxiomC{\C{t : }$~A$}
\RightLabel{$\forall_E$}
\BinaryInfC{\C{(f a) : }$~B[t/x]$}
\DisplayProof
\end{center}

Here the programs as proofs correspondence has a visible impact in the proofs
part of the \mcbMC{} library.  In particular quantified lemmas, being programs,
can be instantiated by simply passing arguments to them.  Exactly as one can
pass \C{3} to \C{addn} and obtain \C{(addn 3)}, the function adding three, one
can ``pass'' \C{3} to the lemma \C{addnC} and obtain a proof of the statement
\C{(forall y, 3 + y = y + 3)}.  Remark that the argument passed to \C{addnC}
shows up in the type of the resulting term \C{(addnC 3)}:  The type of the
\C{addnC} program depends on the value the program is applied to.  Note that
the $\forall$ quantification specifies the type of the bound variable, and that
the $\forall_E$ checks the term $t$ has the right type.

\subsection{Inductive types}

In the previous chapters we used many data types already.  For example we have
seen the type \C{nat} and its constructors \C{O} and \C{S}.  Trough the looking
glasses of the Curry-Howard correspondence \C{O}, being a term of type \C{nat}
can represent a ``proof'' of \C{nat}.  For data types we prefer to say
\emph{inhabited} rather than \emph{proved}, but there is no real difference.

Inductive ``data'' types can be used to represent logical connectives, in
particular the ones we miss so far, the existential quantifier in particular.  
But lets start by the simple conjunction.

\begin{center}
\AxiomC{$A$} \AxiomC{$B$}
\RightLabel{$\wedge_I$}
\BinaryInfC{$A \wedge B$}
\DisplayProof
\hspace{1cm}
\AxiomC{$A \wedge B$}
\RightLabel{$\wedge_E$ (left)}
\UnaryInfC{$A$}
\DisplayProof
\end{center}

The first rule reads: to prove $A \wedge B$ one needs to prove both
$A$ and $B$.  The latter lets one prove $A$ whenever he can prove
the stronger $A \wedge B$ statement.

In Coq such connective can be characterized by the following
inductive definition.

\begin{coq}{name=And}{}
Inductive and (A B : Prop) : Prop := conj (pa : A) (pb : B)
where "A /\ B" := (and A B).
\end{coq}

Remark that the ``data'' type \C{And} is tagged as \C{Prop}, i.e.  we declare
the intention to use it as a logical connective rather than a data type.  The
only constructor \C{conj} that can be used to inhabit \C{and} takes two
arguments, a proof of \C{A} and a proof of \C{B}, faithfully modelling the
logical rule \C{$\wedge_I$}.  Note that \C{A} and \C{B} are parameters of the
inductive definition, similarly to the definition of polymorphic lists
or pairs.  Finally note that the definition of the pair data type is very
similar to the one of the conjunction.

Pattern matching provides a way to express the elimination rule for
conjunction.  In particular the elimination rule is just a projection.

\begin{coq}{name=Ande1}{}
Definition proj1 A B (p : A /\ B) : A :=
  match p with conj a _ => a end.
\end{coq}

Now recall the similarity between $\to$ and $\forall$, where the former is the
simple, non dependent, case of the latter.  If we check the type of
the \C{conj} constructor

\begin{coq}{name=Ande1}{}
Check conj. (* forall A B : Prop, A -> B -> A /\ B *)
\end{coq}

we may wonder what happens if the type of the second argument (i.e. \C{B}) is
made dependent on the value of the first argument (of type \C{A}).
What we obtain is the inductive definition corresponding to the
existential quantification.

\begin{coq}{name=Ande1}{}
Inductive ex (A : Type) (P : A -> Prop) : Prop :=
  ex_intro (x : A) (p : P x).
Notation "'exists' x : A , p" := (ex A (fun x : A => p)).
\end{coq}

The \C{ex\_intro} constructor is the only mean to prove a statement like
\C{(exists n, prime n)}.  In such case the first argument would be a number
\C{n} of type \C{nat} while the second argument would be a proof \C{p} of type
\C{(prime n)}.  The \C{ex} inductive definition is again pretty similar to the
pair, but note that the type of the second component depends on the value of
the first one.  Last note the parameter \C{P} that is a function
representing an arbitrary predicate over a term of
type \C{A}.  Hence \C{(P a)} is the instance of the predicate to \C{a}.  E.g.
the predicate of being an even prime number is expressed as
\C{(fun x : nat => ~~ odd x && prime x)}, and the statement expressing the
existence of such number is
\C{(ex nat (fun x : nat => ~~ odd x && prime x))}.

We quickly see the inductive definition of the disjunction.

\begin{coq}{name=Or}{}
Inductive or (A B : Prop) : Prop :=
  | or_introl (a : A)
  | or_intror (b : B)
where "A \/ B" := (or A B).
\end{coq}

The elimination rule can again be expressed by pattern matching:

\begin{coq}{name=Or}{}
Definition or_ind (A B P : Prop)
  (aob : A \/ B) (pa : A -> P) (pb : B -> P) : P :=
  match aob with
  | or_introl a => pa a
  | or_intror b => pb b
  end.
\end{coq}

The detail worth noting here is that the pattern match construct has two
branches in this case.  Each branch represents a distinct sub proof.  In this
case to prove \C{P} starting from \C{A \\/ B} one has to deal with all
possibilities: prove \C{P} under the assumption \C{A} and prove \C{P}
under the assumption \C{B}.

Typically a logic comes with the $\top$, $\bot$ and $\neg$ constants.
They can be defined as follows.

\begin{coq}{name=TrueFalse}{}
Inductive True : Prop := I.
Inductive False : Prop := .
Definition not (A : Prop) := A -> False.
\end{coq}

Remark that to prove \C{True} one has simply to provide \C{I} that has no
argument.  So proving \C{True} is trivial, and as a consequence eliminating it
provides little help (i.e. no extra knowledge is obtained by pattern matching
over \C{I}).  Conversely it is impossible to prove \C{False}, since it has no
constructor, and pattern matching on \C{False} can inhabit any type, since no
branch has to be provided.

\begin{coq}{name=exfalso}{}
Definition exfalso (P : Prop) (f : False) : P :=
  match f with end.  (* no constructors, no branches *)
\end{coq}

The only base predicate we are left to describe is equality.  The reason we
left is as the last one is that it has a tricky nature.  In particular
equality, as we have seen in the previous chapters, is an open notion
in the following sense.  Terms that compute to the same syntactic expression
are considered as equal, and this is true for any program the user may write.
Hence such notion of equality needs to be somewhat primitive, as 
\C{match} and \C{Fixpoint} are.  One also expects such notion to come
with a substitutivity property: replacing equals by equals must be licit.

The way this internal notion is exposed is via the concept of index
on which an inductive family may vary.

\begin{coq}{name=Eq}{}
Inductive eq (A:Type) (x:A) : A -> Prop := erefl : x = x
where "x = y" := (@eq _ x y) : type_scope.
\end{coq}

This is the first time we see a function type after the \C{:}.
The \C{eq} type constructor takes three arguments: a type \C{A} and
two terms of that type.  Hence one can write \C{a=b} whenever \C{a} and \C{b}
have the same type.
The \C{erefl} constructor takes no arguments, as \C{I}, but its type
annotation says it can be used to inhabit only the type \C{x = x}.
Hence one is able to prove \C{a=b} it only when \C{a} and \C{b} are
equal up to computation.  Conversely by eliminating a term
of type \C{a=b} one discovers that  \C{a} and \C{b} are
equal and \C{b} can be freely replaced by \C{a}.

\begin{coq}{name=EqInd}{}
Definition eq_ind A (P : A -> Prop) x (px : P x) y (e : x = y) : P y :=
  match e with erefl => px end.
\end{coq}

The notion of equality is one of the most intricate aspects of type
theory and out of the scope of this book.  The interested reader
finds an extensive study of this subject in~\cite{hott}.  Later in this
chapter we define and use other inductive families to take advantage
of the ``automatic'' substitution of the implicit equations we see here:
while the type of \C{px} is \C{(P x)} it is accepted as an
inhabitant of \C{(P y)} because inside the \C{match} the term \C{y}
is automatically replaced by \C{x}.

\subsection{Proof commands}

Since proofs are just terms, one can hardly use the term language to
write long proofs.   The proof commands we have seen so far, like
\C{case:} for example, provide a much more compact syntax for proofs and can be explain in terms of the proof terms they generate behind the scenes.

For example \C{case: n} writes for you a \C{match} expression with the right
shape by looking at the type of \C{n}.  If \C{n} is a natural number then there
are two branches, the one for the \C{S} constructor carries an argument of type
\C{nat}, the other one is for \C{0} and binds no additional term.
The \C{case:} tactic is general enough to work with any inductive data type
and inductive predicate.

The \C{apply:} tactic generates an application.  For example \C{apply: addnC}
generates the term \C{addnC t1 t2} by figuring out the correct values of
\C{t1} and \C{t2}, or opening new goals when this cannot be done.

There is a list of proof commands that are shorthands for \C{apply:}
and is only worth mentioning here briefly. \C{split} proves a conjunction
by applying the \C{conj} constructor, \C{left} and \C{right} prove a
disjunction by applying \C{or\_introl} and \C{or\_intror} respectively.
\C{exist t} proves an existentially quantified formula by providing
the witness \C{t} and, later, a proof that \C{t} validated the predicate.
Finally \C{reflexivity} proves an equality by applying \C{erefl}.

The only primitive constructor that remains without an associated proof command
is \C{(fun .. => ..)}.  Operationally what the $\to_I$ and
$\forall_I$ logical rule do is to introduce into the proof context a
new entry.  So far we either expressed this step at the beginning of proofs
by putting such names just after the name of the lemma being prover, or
just after a \C{case:} or \C{elim:} proof command.  Next section
expands this subject covering the full management of the proof context.

\subsection{Intro pattern}

\begin{itemize}

\item Her come the more complex the explaination of \C{=>} and
\C{:}. Warning the \C{:} of \C{apply:} is of a different nature (but
not the one of \C{apply/view:})...

\item General intro patterns: casing brackets \C{[]}, \C{ ->, _},
  \C{/lemma, /(_ x)}. For an example of \C{/lemma}, see \C{/ltnW} in
  \C{ltn_trans}.
\end{itemize}

\subsection{Curry-Howard correspondence for inductive reasoning}

In chapter 1 we have seen the \C{fun} construction of functions and inductive
data types like \C{nat} as well as the pattern matching.  All these
constructions have found a correspondence in the Curry-Howard correspondence.
The only missing piece is recursive programs.  For example
\C{addn} was written by recursion on its first argument, and is a
function taking in input two numbers and producing a third one.
We can write programs by recursion that take in input, among regular  data,
proofs and produce in output other proofs.  Let's look at the
induction principle for natural numbers

\begin{coq}{}{}
About nat_ind.
(* nat_ind :
forall P : nat -> Prop, P 0 -> (forall n : nat, P n -> P n.+1) -> forall n : nat, P n *)
\end{coq}

\C{nat\_ind} is a program that produces a proof of \C{(P n)} for any \C{n}
proviso a proof for the base case \C{(P 0)}, and a proof
of the inductive step \C{(forall n : nat, P n -> P n.+1)}.
Let's write such program.

\begin{coq}{}{}
Fixpoint nat_ind (P : nat -> Prop)
  (p0 : P 0) (pS : forall n : nat, P n -> P n.+1) n : P n :=
  if n is m.+1 then
    let pm (* : P m *) := nat_ind P p0 pS m in
    pS m pm (* : P m.+1 *)
  else p0.
\end{coq}

Every inductive comes with such an induction principle, the job of elim is
to find a suitable value for \C{P} in order to \C{apply} the induction
principle. Typically P can be obtained by taking the goal and generalizing
it.  Complex \C{elim} like \C{elim: {2}n (leqnn n)}.

\subsection{Axioms of the logic we work with}

Not all valid reasoning principles can be expressed as programs.
For example excluded middle can be proved by a program only when
the predicate is decidable, i.e. when we can write in Coq a program
to \C{bool} that decides it.
Full excluded middle can only be \emph{axiomatized}, i.e. assumed globally.

In \mcbMC{} we stay axiom free.  This makes the library compatible with as many
extra axioms as possible (i.e. not all combinations of axioms are consistent,
hence picking one may rule out others).  Sometimes this is done by Goedel monad,i.e. confining the use of excluded middle inside a box one can open only to prove decidable statements..., sometimes by reworking the math in a weaker setting like RCF (less reals but enough).


\section{Boolean reasoning, \C{Prop} reasoning, going back and forth}



\subsection{Motivations for boolean reflection}

As described on the picture 1:
\begin{itemize}
\item We now have two ways to express logical statements, and we just
  saw they are different (EM).
\item The first one is \C{bool},
  with \C{true, false : bool : Type}.
  It is a datatype with case analysis and computation (hence automation)
  available, which is good. And we can use \C{bool} to express some
  statements like \C{b1 && b2 = b2 && b1}. The other is \C{Prop}, with
  \C{? : P : Prop}. It is a sort, inhabited by statements, which are
  (possibly) themselves inhabited by proofs built by deduction. The
  later is necessary as bool does not capture all the statements we
  would like to work with. But can we bring the good points of
  \C{bool} in the \C{Prop} world when it is possible? For this purpose
  we need:
  \begin{itemize}
    \item We need to send \C{b1 && b2} to \C{Prop}: we do it via
      \C{is_true}.
    \item We need to relate \C{&&} and \C{/\\}.
  \end{itemize}
  By the way, in \C{bool}, the only proof term is \C{erefl true} (this
  is called UIP), which comes handy when crafting data structures that
  pair a data with a spec (comprehension style). We do not comment
  more but this will be expanded in chapter~\ref{ch:sigmabool}.
\end{itemize}

First tool, that we already used: the \C{is_true} coercion, which
gives the illusion of inhabiting a \C{bool}.
Hence one can use rewriting.

\subsection{Equivalences between booleans and \C{Prop} statements}

As described on the picture 2:
\begin{itemize}
\item The intented meaning is \C{b = true <-> P}. But this is not
  convenient in practice to model case analysis on \C{P}, which
  becomes logically justified. So we are going to craft an alternate
  formulation of the same fact: just like in math it is common to
  present several equivalent forms of a same definition, to be used in
  different contexts.

\item Second try: \C{(b = true /\\ P) \\/ (b = false /\\ ~ P)}. Nicer
  but we still have some remaining bureaucracy with conjunctions.

\item Next try: an ad hoc inductive predicate:

\begin{coq}{}{}
Inductive |*reflect*| (P : Prop) (b : bool) : Prop :=
|ReflectT (p : P)    (e : b = true)
|ReflectF (np : ~ P) (e : b = false)
\end{coq}

\marginnote{We will put \C{reflect} in \C{Type} only in Chapter
  \ref{ch:sigmabool}, when describing \C{insub}.}
Better to drive proofs with EM (however we will later refine this one
once more, and this refinement will be relevant to all the spec
lemmas).

\item First examples: \C{andP}, \C{orP}, \C{implyP} with proofs by
  case analysis of the truth tables.
\end{itemize}
These are called views.

\subsection{Prove and use views}

\begin{itemize}

\item Example of \C{eqnP}, that is \C{eqP} specialized to
  \C{nat}. Proof using \C{(iffP idP)}. Explain \C{iffP} and \C{idP} is
  the dummay case.

\item Another example: Simplified instance of \C{inj_eqAxiom} in
  section  \C{TransferEqType} of \C{eqtype.v}, with \C{nat} as
  codomain. Proof with \C{(iffP eqnP)}.

\item Using views, with tactics. First, example of using \C{eqnP} in
  an intro pattern, like \C{=> /eqnP ->}. Note that the direction in
  which the view should be used has been guessed
  automatically. Explain which adapter has been inserted (as a hint).

\item Then show the cute proof of \C{leq_total}, which features
  \C{apply/implyP}.

\item Finally, show \C{case/orP: (leq_total n m)}. May be a first
  simple and dummy example. Then one possibility is
  to show a simplified version of the proof of \C{leq_max}, removing
  the \C{without loss}. This features \C{case/orP: (leq_total n2 n1)}
  and \C{rewrite (maxn_idPl le_n21)} which uses the \C{elimT} coercion.
  This could be reused in the next section, to illustrate \C{wlog}.


\item There are more adaptors than \C{introT, introF, elimT, elimF},
  in particular with negations \C{elimN,...} and \C{apply/v1/v2}.

\end{itemize}

Rmk: The view feature in the tactic language is there to combine an
action (tactic or intro pattern) with a change of world. Order in
which we could introduce it: \C{/eqP ->}, \C{/andP [h1 h2]},
\C{/orP [h1 | h2]}. May be mention \C{=> /v1 /v2} as a side remark.

% It is the fragment of decidable stuff (EM as case).
% It is a concrete data type on which you can program (SSR), automation by
% computation.

% % \begin{coq}{name=Ex}{}
% % Lemma muln_eq0 m n :
% %   ((m * n = 0) -> (m = 0) \/ (n = 0)) /\
% %   ((m = 0) \/ (n = 0) -> (m * n = 0))
% % Proof.
% % Qed.
% %
% % Lemma leq_mul2l m n1 n2 :
% %   (m * n1 <= m * n2) = (m == 0) || (n1 <= n2).
% % Proof.
% % Qed.
% % \end{coq}



% \begin{coq}{name=Ex}{}
% Lemma leq0n n : (0 <= n) (* = true *).
% \end{coq}

% NOt everything can be in bool, e.g. exists or a real reasoning by cases
% on a disjunction. Inductives give you the tree structure in natural
% deduction, not bools.

% \begin{coq}{name=Ex}{}
% Lemma ...
% case/orbP : (leq_total n m)
% \end{coq}

% We need lemmas to relate

% \begin{coq}{name=Meaning of reflect}{}
% Definition reflect P b : Prop :=  b -> P /\ P -> b

% Lemma orbP p q : reflect (p \/ q) (p || q).
% \end{coq}

% so frequent and so many variations that we have proper infrastructure like
% being able to invoke views everywhere and have 1 view per connective (negate or
% not...).

% \begin{coq}{}{}
% Lemma introT  : P -> b.            Proof using Pb. exact: introTF true _. Qed.
% Lemma introF  : ~ P -> b = false.  Proof using Pb. exact: introTF false _. Qed.
% Lemma introN  : ~ P -> ~~ b.       Proof using Pb. exact: introNTF true _. Qed.
% Lemma introNf : P -> ~~ b = false. Proof using Pb. exact: introNTF false _. Qed.
% Lemma introTn : ~ P -> b'.         Proof using Pb'. exact: introTFn true _. Qed.
% Lemma introFn : P -> b' = false.   Proof using Pb'. exact: introTFn false _. Qed.
% \end{coq}

% \subsection{how to use reflect lemmas as tactic decorators}

% we make examples with andP orP negP in move/P, apply/P, case/P.

% we explain the hint view, plus the extra impl arguments.

% some view can be partial, A -> B -> reflect B c.

% \subsubsection{The view mechanism in intro pattern}

%   a == b \&\& bb

% views applied to top, inline destructuring and subst:
%   => /andP[/eqP-> pb] ->.

% Fwd and backward declarative steps.
% have : P x := ... H ...
% suff.

% Handling symmetries:
% gen have: x Hx / P x.

% managing large goals and contexts: set, -/x /x

% help the reader with typographical comments, like leaving an empty
% line in latex (here we use bullets).

Not everything needs to be a tactic, the logic is powerful
enough to express special connectives that induce a line of
reasoning. Hence the next section.

\section{Advanced, practical, statements}

general talk about the fact that statements/definition do matter: not only for
their meaning but also because they have implications on the
usability/practicality in the rest of the library.  At least two classes of
techniques, the ones based on the logic (bool refl, reflect, classically, order
of forall when instantiation done via CH) and the ones based on the support of
the prover (implicit args, type inference CS, Hint Resolve).

More in general, this has to be mentioned in the main intro of the book, here
we revise the idea.

\subsection{Inductive specs with indices}
What we did for reflect, an ad hoc connective, to model a line of reasoning is
a recurrent pattern in the \mcbMC{} library: the ``spec'' predicates.  Specs
are particularly handy  because inductive predicates, via their elimination
rule, accesses a special feature of the type theory of Coq: implicit equations
/ automatic substitution.

If we look at reflect, and its use, one is very likely to substitute b
for its value in each branch of the case.

\begin{coq}{}{}
Inductive |*reflect*| (P : Prop) (b : bool) : Prop :=
|ReflectT (p : P)    (e : b = true)
|ReflectF (np : ~ P) (e : b = false)
\end{coq}

This alternative formulation makes the equation implicitly stated and
also automatically substituted during case analysis.

\begin{coq}{}{}
Inductive |*reflect*| (P : Prop) : bool -> Prop :=
|ReflectT (p : P)    : reflect P true
|ReflectF (np : ~ P) : reflect P false
\end{coq}

Here the second argument of \C{reflect} is said to be an \emph{index}
and it is allowed to vary depending on the constructor: \C{ReflecT} always
builds a term of type \C{(reflect P true)} while \C{ReflectF} builds
a term of type \C{(reflect P false)}.
Example:

\begin{coq}{}{}
case: (andP a b) => [ab|nab]
\end{coq}

\begin{coqout}{}{width=6cm}
a, b : bool
============================
a && b ==> (a == b)
\end{coqout}
\begin{coqout}{}{width=6cm}
a, b : bool
ab : a /\ b
============================
true || a == b

subgoal 2 is:
false || a == b
\end{coqout}

Every time a term of type \C{(reflect (a /\\ b) (a && b))} is eliminated, the
following happens:
\begin{enumerate}
\item 2 subgoals are generated
\item in the first one (corresponding to \C{ReflectT}) a new hyp is
  available (\C{(a /\\ b)}) and all occurrences of the boolean
  expression \C{(a && b)} are replaced by \C{true} (the value of the index
  for \C{ReflectT}).
\item in the first one (corresponding to \C{ReflectF}) a new hyp is
  available (\C{\~(a /\\ b)}) and all occurrences of the boolean
  expression \C{(a && b)} are replaced by \C{false}, the value of
  the index for \C{ReflectF}.
\end{enumerate}

The second goal becomes trivial, hence we can finish

\begin{coq}{}{}
case: (andP a b) => [[-> ->] | //]
\end{coq}

An even better tactic is

\begin{coq}{}{}
case: (andP _ _) => [[-> ->] | //]
\end{coq}

In this case the boolean expression replaced by true/false is \C{(_ && _)}.
Trailing \C{_} are also added automatically by \C{case} leading to the
idiomatic

\begin{coq}{}{}
case: andP => [[-> ->] | //]
\end{coq}

Note the similarity between matching \C{(_ && _)} and the job of rewrite.

On the same line, we present \C{leqP} that still models a case split but
this time specialized to the order relation (and its negation) substituting
both with true/false.  Just mention \C{ltngtP}, that is the proof that
a boolean predicate (because of its 2 valued nature) can model a concept
with 3 cases as well via this spec mechanism.  Or that the same concept,
depending on the proof you need to do, may benefit from different natures
of case splits.

\mantra{the structure of the proof shall not be driven by the syntax (head
symbol) of the definition/predicate under study but by the view/spec you apply
to it}

Just show a use of \C{ifP} and/or \C{ifPn} and the fact that its pattern is
quite smart/small wrt the expression it typically handles (3 chars in place of\\
\C{(if cond then branch else alternative)}).

Also show the adaptor altP, that is particularly useful with eqnP and boolP.

\subsection{TBD: dependent elimination (**)}

explain return match clause.  Maybe just the syntax we provide here and
there, the / annotation for elim/case, and then point to other texts explaining
the thing.  \mcbMC{} does not use such thing but for spec.

\subsection{Other tools to craft good statements}

\begin{itemize}
\item Use macros (\C{left_commutative} or notations like
\C{\{in A, bijective f\}}, and3, ...)
\item Use naming conventions
\item Classically (do not insist too much)
\item iff (\C{AGM}): find examples?
\item Tuning of implicit arguments
\end{itemize}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Towards real proofs}

Real proofs also deal with structure (readability, naming, context handling and
robustness), repetitions (like symmetry).  This section works out a few
examples illustrating good practices and specific proof
commands. Start with infinity of primes (and the two lemmas it relies
on), in order to introduce \C{have} and to see how one can work on a
goal, even in implicative shape. Then this one:

Example:
$$
\forall n_1, n_2, m, \quad m \le max(n_1,n_2)
\Leftrightarrow m \le n_1 \textrm{ or } m \le n_2
$$

The proof goes like that: without loss of generality we can assume that
$n_2$ is greater or equal to $n_1$, hence $n_2$ is the maximum between
$n_1$ and $n_2$.  Under this assumption it is sufficient to check
that $m \le n_2$ holds iff either $m \le n_2$ or $m \le n_1$.
The only non trivial case is when we suppose $m \le n_1$ and
we need to prove $m \le n_2$ which holds by transitivity.\hfill$\square$

As usual we model double implication as an equality between two
boolean expressions:

\begin{coq}{}{}
Lemma leq_max m n1 n2 : (m <= maxn n1 n2) = (m <= n1) || (m <= n2).
\end{coq}

The proof uses the following (new) lemmas.  Pay attention to
\C{orb_idr}.

\begin{coq}{}{title=tools}
Lemma orb_idr (a b : bool) : (b -> a) -> a || b = a.
Lemma maxn_idPl {m n} : reflect (maxn m n = m) (m >= n).
Lemma leq_total m n : (m <= n) || (m >= n).
\end{coq}

Our first take takes no advantage of the symmetry argument and
begins by reasoning by cases on the order relation,
we name the resulting hyp with a meaningful name and close to
the proof step that generates it (to ease tracking who adds what).

\begin{coq}{}{}
Proof.
case/orP: (leq_total n2 n1) => [le_n21|le_n12].
  rewrite (maxn_idPl le_n21) orb_idr // => le_mn2.
  by apply: leq_trans le_mn2 le_n21.
rewrite maxnC orbC.
rewrite (maxn_idPl le_n12) orb_idr // => le_mn1.
by apply: leq_trans le_mn1 le_n12.
Qed.
\end{coq}

alternative with a more declarative have (probably useless).

\begin{coq}{}{}
Proof.
have /orP[le_n21|le_n12] := leq_total n2 n1.
  rewrite (maxn_idPl le_n21) orb_idr // => le_mn2.
  by apply: leq_trans le_mn2 le_n21.
rewrite maxnC orbC.
rewrite (maxn_idPl le_n12) orb_idr // => le_mn1.
by apply: leq_trans le_mn1 le_n12.
Qed.
\end{coq}


2 goals, hence indentation.

\begin{coqout}{}{title=Output line 3,width=8cm}
2 subgoals
m, n1, n2 : nat
le_n21 : n2 <= n1
============================
(m <= maxn n1 n2) = (m <= n1) || (m <= n2)

subgoal 2 is:
(m <= maxn n1 n2) = (m <= n1) || (m <= n2)
\end{coqout}

The first goal is simplified by
reqriting with the \C{maxn_idPl} view (recall elimT).  Then
\C{orb_idr} trivializes the main goal and generates a side condition
with an extra hyp we name \C{le_mn2}

\begin{coqout}{}{title=Output line 4a,width=6.7cm}
2 subgoals
m, n1, n2 : nat
le_n21 : n2 <= n1
============================
(m <= n1) = (m <= n1) || (m <= n2)

subgoal 2 is: ...
\end{coqout}
\begin{coqout}{}{title=Output line 4b,width=5.3cm}
2 subgoals
m, n1, n2 : nat
le_n21 : n2 <= n1
le_mn2 : m <= n2
============================
m <= n1

subgoal 2 is: ...
\end{coqout}

Line 3 combines by transitivity the two hyps to conclude.
Since it closes the proof branch we use the prefix \C{by}
to asserts the goal is solved and visually signal the end of the paragraph.
(maybe use \C{exact}).

Line 4 commutes the max and or, hence we can copy paste the first paragraph
also to close the second goal.

Next take factorizes a generalization of goal to take care of symmetry.

\begin{coq}{}{}
Lemma leq_max m n1 n2 : (m <= maxn n1 n2) = (m <= n1) || (m <= n2).
Proof.
have th_sym x y: y <= x -> (m <= maxn x y) = (m <= x) || (m <= y).
  move=> le_n21; rewrite (maxn_idPl le_n21) orb_idr // => le_mn2.
  by apply: leq_trans le_mn2 le_n21.
by case/orP: (leq_total n2 n1) => /th_sym; last rewrite maxnC orbC.
Qed.
\end{coq}

Last line instantiates \C{th_sym} \emph{in each branch} using the corresponding
hypothesis on \C{n1} and \C{n2} generated by the case analysis.

\begin{coqout}{}{}
2 subgoals
m, n1, n2 : nat
th_sym : forall x y : nat,
         y <= x -> (m <= maxn x y) = (m <= x) || (m <= y)
============================
(m <= maxn n1 n2) = (m <= n1) || (m <= n2) ->
(m <= maxn n1 n2) = (m <= n1) || (m <= n2)

subgoal 2 is:
(m <= maxn n2 n1) = (m <= n2) || (m <= n1) ->
(m <= maxn n1 n2) = (m <= n1) || (m <= n2)
\end{coqout}

This is exactly what is needed in the first branch of the case analysis.
The last subgoal just requires commuting \C{max} and \C{||}.

We prefer to get rid of the easiest part of the proof first, thus
keeping the main branch of the proof non indented. In this case
it amounts to begin by showing why \C{th_sym} suffices to prove the main
goal.

\begin{coq}{}{}
Lemma leq_max m n1 n2 : (m <= maxn n1 n2) = (m <= n1) || (m <= n2).
Proof.
suff th_sym x y: y <= x -> (m <= maxn x y) = (m <= x) || (m <= y).
  by case/orP: (leq_total n2 n1) => /th_sym; last rewrite maxnC orbC.
move=> le_n21; rewrite (maxn_idPl le_n21) orb_idr // => le_mn2.
by apply: leq_trans le_mn2 le_n21.
Qed.
\end{coq}

Final step, wlog lets us not repeat the goal and abstract it, but
just mention the extra assumption we can take without loosing
generality.

\begin{coq}{}{}
Lemma leq_max m n1 n2 : (m <= maxn n1 n2) = (m <= n1) || (m <= n2).
Proof
wlog le_n21: n1 n2 / n2 <= n1 => [th_sym|].
  by case/orP: (leq_total n2 n1) => /th_sym; last rewrite maxnC orbC.
rewrite (maxn_idPl le_n21) orb_idr // => le_mn2.
by apply: leq_trans le_mn2 le_n21.

by rewrite /maxn le_n21 orb_idr // => le_mn2; apply: leq_trans le_mn2 le_n21.

(* then, recalling apply adds _ we can avoid naming/mentioning le_mn2*)
by rewrite (maxn_idPl le_n21) orb_idr // => ?; apply: leq_trans le_n21.

(* or even better using lemmas in intro pattern + auto generalization
   plus rewriting an is_true... or maybe not *)
by rewrite (maxn_idPl le_n21) orb_idr // => /leq_trans->.
Qed.
\end{coq}

For wlog, say that the block naming \C{th_sym} is optional and frequently
omitted (at the cost of less explicit script).

\begin{coq}{}{}
Lemma edivnP m d :
 let ed := edivn m d in
   ((d > 0) ==> (ed.2 < d)) && (m == ed.1 * d + ed.2).
Proof.
move E: (edivn m d) => ed /=.
case: d => [|d /=] in E *; first by rewrite -E eqxx.
rewrite /edivn /= in E.
rewrite -[m]/(0 * d.+1 + m).
elim: m {-2}m 0 (leqnn m) E => [|n IHn] [??<-|m] //= q le_mn.
rewrite subn_if_gt; case: (leqP d m) => [le_dm|lt_md <- /=]; last first.
  by rewrite ltnS lt_md eqxx.
have le_mdn : m - d <= n by rewrite (leq_trans (leq_subr d m)).
by move/(IHn _ _ le_mdn); rewrite mulSnr -addnA -subSS subnKC.
Qed.
\end{coq}

May be a real 10 lines proofs would help here. Lemmas about Euclidean
division?
 \begin{itemize}
\item Proofs have a structure, to ensure readability (in a certain
  sense) and maintenance; we list here some tools and good practice.
\item Declare intermediate steps: forward (\C{have}), backward
  (\C{suff}), symmetries (\C{gen have} and mention \C{wlog}),
abbreviations \C{set}
\item: Punctuation to help the reader: bullets, indentation,
  terminating tactics.
\item Good practices: local/early error detection, checkpoints via
  \C{have}, naming policies (lemmas, but also hypotheses and bound
  variables), robust rewrite.
\item keep context clean?
\end{itemize}

\section{STOP HERE}

THIS CHAPTER IS ABOUT METHODOLOGY, plus introduces the other logical
connectives. May be merged into the previous chapter.


% Where one learns to do proofs.
% Boolean reflection in practice, views, discussion on the definition of leq,
% proofs on things defined in the
% previous chapter, associated tactics, exercises on prime, div,
% binomial, etc.
%
% spec? A new vernacular to declare specs without typing coinductive and
% by writing explicitly the equations.

Discussion prop/bool, intuitionism, extraction (we should be able to
avoid talking about impredicativity, but use Prop for computationally
irrelevant).


\begin{itemize}
\item can we specify all we have written so far using just = and forall? No.
	Example dvdn needs exists to be specified
\item exists, and, or, neg, False, True as inductives (again CH style)
\item related tactics: split, left, right,exists,case
\end{itemize}

Anyway to take advantage of computation (ssr style) we want to
work with bool as much as possible:

\begin{itemize}
\item reflect is the right way to write iff, <->, specialized to bool
	so that the proof language recognizes it and offer a bit more ergonomic
\item is-true
\item infrastructure for reflect: iffp, altp
\item no split if goal is \&\& (metodology)
\end{itemize}

Writing good statements

\begin{itemize}
\item = as iff for bool, because rewrite is easy to use
\item and3p, spec (drive your proof),
\item advanced stuff: classically P instead of not-not P (can be
  skipped for beginners)
\item in general good quantifications and implicit arguments and good library
	makes it possible to work without evars
\item . \C{<=} . ?= iff .
\end{itemize}

Statements do also occur in the middle of proofs.  There we have many ways to
write them compactly, wlog and have.

Comparison with other possible ways of writing properties:
\begin{itemize}
\item impact of le v.s. leq in a proof
\end{itemize}

In this chapter we should distill a description of our
systematic-reactions, reflexes, to typical situations a
beginner would screw up. In fact it would be great to explain here the
mix of Gallina (unless, classically, etc.) and of tactics (wlog,
have,...) that lead to a convenient modelling of the math prose.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\mcbLEARN{Declaring implicit arguments}
\mcbREQUIRE{Canonical}
\mcbPROVIDE{stating lemmas}
\mcbLEVEL{2}
\mcbsection{Declaring implicit arguments}\label{sec:declaringimpl}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

here we describe how to choose which arguments are implicit,
that one has to think ahead how  a lemma is used and hence
which data type inference has at hand.  Also that the order
of quantifiers is relevant.
\begin{itemize}
\item lemmas: fwd/backward reasoning
\item equations, look at the concl too, free vars are abstracted
\item compare with eapply style
\end{itemize}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\mcbLEARN{Coercions}
\mcbREQUIRE{}
\mcbPROVIDE{}
\mcbLEVEL{1}
\mcbsection{Notational aspects of specifications}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% Even if the main way to extend the type inference algorithm
% is via Canonical Structures, another mechanism is available
% and used all over the library, even if it plays a minor role.
% The language of Canonical Structures lets one program how the value of
% an implicit argument can be synthesized, but can hardly be used to
% explain \Coq{} how to ``fix'' an ill-typed term written by the user.

When a typing error arises, it always involves three objects:
a term \lstinline/t/, its type \lstinline/ity/ and the type
expected by its context \lstinline/ety/.  Of course, for this
situation to be an error, the two types \lstinline/ity/ and
\lstinline/ety/ do not compare as equal.
The simplest way one has to explain \Coq{} how to fix \lstinline/t/,
is to provide a functional term \lstinline/c/ of type
\lstinline/(ity -> ety)/ that is inserted around \lstinline/t/.
In other words, whenever the user writes \lstinline/t/ in a context
that expects a term of type \lstinline/ety/, the system instead of
raising an errors replaces \lstinline/t/ by \lstinline/(c t)/.

A function automatically inserted by \Coq{} to prevent a type
error is called \emph{coercion}.
The most pervasive coercion in the \mcbMC{} library is
\lstinline/is_true/ that lets one write statements using boolean
predicates.\marginnote{I guess in a way or another is true has
already been introduced}.

\begin{coqdef}{name=istrue}
Lemma example : prime 17.
Proof.
Set Printing Coercions. Redirect "g1" Show.
by [].
Qed.
\end{coqdef}
\begin{coq}{def=istrue}{title=Coercion \lstinline/is_true/,width=6cm}
Lemma example : prime 17.
Proof.  by [].  Qed.
\end{coq}
\coqrun{name=r3}{ssr,istrue}
\begin{coqout}{run=r3;out=g1}{title=Goal after line 3,width=6cm}
1 subgoal

============================
is_true (prime 17)
\end{coqout}

The statement of the example is processed by type inference,
it is enforced to be a type, but \lstinline/(prime 27)/ is actually
a term of type \lstinline/bool/.  Early in the library the
function \lstinline/is_true/ is declared as a coercion from
\lstinline/bool/ to \lstinline/Prop/ and hence is it inserted
by \Coq{} automatically.

\begin{coq}{name=istruedef}{}
Definition is_true b := b = true.
Coercion is_true : bool >-> Sortclass. (* Prop *)
\end{coq}

Another coercion that is widely used injects booleans into naturals.
Two examples follow.
\marginnote{Mention Kronecker delta as a math example of the same hack}

\begin{coq}{name=natofbool}{}
Variable T : eqType.
Fixpoint count (a : pred T) (s : seq T) :=
  if s is x :: s' then a x + count a s' else 0.
Lemma count_uniq_mem (s : seq T) x :
  uniq s -> count_mem x s = (x \in s).
\end{coq}
\coqrun{name=ex}{ssr,istruedef,natofbool,abort}

In line number 2 the term \lstinline/(a x)/ is a boolean.  The
\lstinline/nat_of_bool/ function is automatically inserted to turn
\lstinline/true/ into 1 and \lstinline/false/ into \lstinline/0/.
Similarly, in the last line the membership test is turned into
a number, that is shown to be equivalent to the count of any
element in a list that is duplicate free.

Another example of a coercion that is related to the running example
of the current chapter is \lstinline/sort/.  Typically the projection
of a record type extracting the data type is declared as a coercion
letting one state generic theorems like in the following example.

\begin{coqdef}{name=sotc}
Lemma example (e : eqType) : forall x y : e, x == y -> y == x.
\end{coqdef}
\begin{coq}{def=sotc}{}
Lemma example (e : eqType) : forall x y : e, x == y -> ...
\end{coq}
\coqrun{name=r5}{ssr,sotc,abort}

Here the type of \lstinline/x/ and \lstinline/y/ is
\lstinline/(sort e)/ and not \lstinline/e/ as the user initially wrote.
Indeed \lstinline/e/ is a term (of type \lstinline/eqType/) while
the \lstinline/forall/ quantification expects a type after the
colon.  The \lstinline/sort/ function mapping an \lstinline/eqType/
into a \lstinline/Type/ is inserted automatically.

Coercions are composed transitively.

\begin{coq}{name=b2z}{}
Definition zerolist n := mkseq (fun _ => 0) n.
Coercion zerolist : nat >-> seq.
Check 2 :: true == [:: 2; 0].
\end{coq}
\coqrun{name=r6}{ssr,b2z}

For the convenience of the reader we list here the most widely
used coercions. there are also a bunch on Funclass not listed
and elimT surely deserves some explanation.

\noindent
\begin{tcolorbox}[colframe=blue!60!white,before=\hfill,after=\hfill,center
	title,tabularx={l|l|l},fonttitle=\sffamily\bfseries,title=Coercions]
coercion & source & target \\ \hline
\lstinline/Posz/ & \lstinline/nat/ & \lstinline/int/ \\
\lstinline/nat_of_bool/ & \lstinline/bool/ & \lstinline/nat/ \\
\lstinline/elimT/ & \lstinline/reflect/ & \lstinline/Funclass/ \\
\lstinline/isSome/ & \lstinline/option/ & \lstinline/bool/ \\
\lstinline/is_true/ & \lstinline/bool/ & \lstinline/Sortclass/ \\
\hline
\end{tcolorbox}

\marginnote{This may go in Chapter 1}
Another device that is used to help type inference is the
\lstinline/Implicit Types/ directive.  This directive lets
one attach a default type to variable names.

\begin{coq}{name=itype}{title=Example of \lstinline/Implicit Types/}
Implicit Types m n : nat.
Check forall m n, n == m.
\end{coq}
\coqrun{name=r7}{ssr,itype}

In the example above the statement we \lstinline/Check/ does not
contain enough information alone to be well types.  The overloaded
\lstinline/==/ notation needs the terms to which it is applied to
have a type for which a \lstinline/Canonical Structure/ is declared.
Even if we did not annotate \lstinline/n/ and \lstinline/m/ with a
type, the directive on the first line does it for us.

The reader already familiar with the concept of coercion
may find the presentation of this chapter nonstandard.
Indeed coercions are usually presented as a device to model
subtyping in a theory that, like \mcbCIC{}, does not
feature subtyping.  As we will see in Chapter~\ref{ch:hierarchy}
the role played by coercions is in the modelling of the hierarchy
of algrabraic structure is minor.  Indeed what is hard is not to
forget some fields of a structure to obtain a simpler one.  What
is hard is to reconstruct the missing fields of a structure
or compare two structures finding the minimum super structure.
These tasks are mainly implemented using canonical structures.
