% vim:set tw=70:
% vim:set spell:
% vim:set errorformat="":
\begin{coqdef}{name=ssr}
Require Import Ssreflect.ssreflect.
From Ssreflect Require Import ssrfun ssrbool ssrnat eqtype fintype seq div prime.
Set Implicit Arguments.
Unset Strict Implicit.
Unset Printing Implicit Defensive.
Set Asymmetric Patterns.
\end{coqdef}
\begin{coqdef}{name=abort}
Abort.
\end{coqdef}
\begin{coqdef}{name=show1}
Redirect "g1" Show.
\end{coqdef}
\begin{coqdef}{name=require-tuple}
Require Import tuple.
\end{coqdef}

\Chapter{Subtypes: terms with extra properties}{}\label{ch:sigmabool}

\begin{itemize}
\item mixing data and proofs is possible
\item get richer objects out of already existing ones without
	redefining new objects
\item need to inherit the theory of base objects
\item handy to infer the extra property (easy to form)
\item exzample 1 : ordinals (fintype)
\item impact of ordinals on bigop
\item finfun and matrixes
\item example 2 : polynomials (need ringType)
\end{itemize}

to build a large library one needs to reuse as much as possible what
is available and build incrementally.  so far inductive types have
been the only way to define a new object, while bool predicates the
common way to define predicates.

in CIC proofs are terms, so one can put together objects and proofs
of properties to obtain new objects.  this operation is expensive in
the sense that imposes on the user extra work whenever he has to build
an inhabitant, so hat to be use with care.  some techniques to ease
the creation of such types and of such inhabitants are provided. also
crafting a new type lets one enrich its type to expose to the type
system some piece of info.  type inference can use so to let one write
very concisely statements.

examples are the type of ordinals and the type of polynomials.

ordinals expose the size in their types and are finite.  This is for
example useful to leave size constraints implicit in expressions about
matrixes.  If a matrix is a function from in*im to x, then we can't
possibly address elements in the matrix outside its dimensions, and we
can write : determinant, biop switch sum prod.

it is crucial that the theory is inherited. coercions and CS help
here.  organizing the theories around a hirarchy in next chapter.
ordinals inherit arithmetic.
polynomials are lists of coefficients. one typically wants a canonical
representation, so that equal polymonials can be proved eq.
we can easily impose such proposerty in the proposed approach.
we inherit all the operations on lists, and all the theory of eqType,
via the subtype machinery.


\section{xxxxxxxxxxxxxx old xxxxxxxxxxxxxxxxx}

% 0- tuple: reuse the theory of seq, + the invariant on the length
% 1- seq is eqType -> tuple is eqType by <:
% 2- new interface fintype
% 3- example: I_n (mainly to talk about finType)
%    - is a eqType by <:
%    - but we prove that it is a finType too
% 4- we refine 'I_n/tuple do define ffun
%    - is an eqType by <:
%    - is a fintype with a proof from tuple (like 'I_n)
% 5- we refine ffun to 'S_n that
%    - is eqType by <:
%    - is finType by <:
% 6- spoiler for ch6: if we start an abstract theory on T:finType
%    then == also works!! Magic?
% ??- finset/card/enum, \in, \big_(x \in A)
% ?? use strings of pearls as an example (may be)


Wild notes: the purpose of this chapter is to stress what is truely
important and perennial, as opposed to technicalities too much tied to
the present technology implemented in Coq... tentative list:
\begin{itemize}
\item Organization of classes from mixins
\item Rationale of instances and heritage
\item Respective roles of coercions and type inference
\item Visibility of coercions and notations, which motivate the
  \C{Module Import Export} discipline.
\item Slightly advanced readers should be taught Pack, and which edges
  are to be added to the graph of the hierarchy when a new
  structure is added.
\end{itemize}


Organizing, structuring, knowledge is a delicate task. A successful
path followed in mathematics is algebraization. The focus shifts
towards the relations among classes of similar objects and general
theories that apply to all the objects of a given class.
An experienced mathematician can use such structured
knowledge by recognizing that his object of study belongs to a know
class and hence a string of generic results holds for it.

Given how popular this approach is in modern mathematics we can't
really avoid modelling it in the \mcbMC{} library.  It is well known that
modelling structured knowledge, like algebra, inside a proof assistant
is hard, and in our case an extra problem arises: we need such
modelling to scale up to a large library. This extra difficulty turns
out to actually provide good directions for finding the right
techniques.  Indeed, computer science too tackled the task of
organizing large libraries of computer code, and did it for decades.
One of the outcomes is object oriented programming (OOP).
It is hard to give a comprehensive presentations of all the key
principles behind OOP, but some of them match pretty well our needs.
OOP advocates the use of interfaces (typically called classes) to
organize a complex system into smaller parts, it enables one
to declare relations between the interfaces to reuse and
specialize them (inheritance).  Finally OOP language provides some
magic glue to make all that work together, enabling one to mix and
match data and code belonging to different but related interfaces, as
long as it ``makes sense''.  What makes sense really means depends too
much on the programming language details to be explained in general,
and if we draw a parallel with mathematics this pretty much
corresponds to the silent check an expert eye does when reading a text
mixing different theories.
When we look at functional programming languages, like the one
provided in \mcbCIC{}, the aspect of this magic glue that is the most
relevant here is \emph{sub-type} polymorphism.

Its intuitive meaning can be explained with an example.  Take the two
related notions (types) of a simple point \lstinline/P/ on the plane
and the more refined concept of coloured point \lstinline/CP/.  To
maximize code reuse we want the function \lstinline/move/, that shifts
a point, to also work on points that happen to have a color.
If we assign the type \lstinline/P -> P/ to such function, then we
can't use it on a coloured point \lstinline/c/, because
\lstinline/(move c)/ would be ill-typed.
We could try to assign to \lstinline/move/ the polymorphic type
\lstinline/$\alpha$ -> $\alpha$/ (for all $\alpha$), but that would
fail too. The function \lstinline/move/ really needs to know something
about its input, like its coordinates, in order to perform.  The
polymorphic type requires the implementation of \lstinline/move/
to bee too generic, and
in practice we can only inhabit the type
\lstinline/$\alpha$ -> $\alpha$/ with the identity
function,\footnote{there is even a theory about that, called
parametricity} that does not exactly move the point as one expects.
Sub-type polymorphism lets one assign a polymorphic type to
\lstinline/move/ where the type variable $\alpha$ is constrained to be
``at least'' a point, a constraint satisfied by all coloured points.

The record data types introduced in the previous chapter played the
role of expressing a relation between a type and a function (the
comparison one).  Being records dependently typed, their fields can
also carry proofs testifying that some property holds on the value of
the other fields.  This in turns makes it possible to use records to
model precisely the concept of interface.  Programmable type inference
can then serve as the glue that links an object with all the
interfaces it satisfies.

In this chapter we focus on the tricky construction of
\emph{dependent pairs packaging data and proofs}.  To take
full advantage of this construction while doing proofs we
implement the glue typical of OOP languages by \emph{programming
type inference}.
Inheritance among interfaces will be only sketched here: a complete
treatment of it requires a full chapter, the next one.

Now \lstinline/eqType/ can be seen as an interface to access a
theory of results.  Indeed we can finally state (and prove) the
Hedberg theorem in its full generality.

\begin{coq}{name=hedberg}{title=Hedberg}
Theorem eq_irrelevance (T : eqType) (x y : T) : forall e1 e2 : x = y, e1 = e2.
\end{coq}
\coqrun{name=hedberg}{ssr,hedberg,abort}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\mcbLEVEL{1}
\mcbLEARN{panel of options}
\mcbREQUIRE{dependent product}
\mcbPROVIDE{sigma with bool predicate}
\mcbNOTES{}
\mcbsection{The many ways to define a ``sub-type''}

% Even if we leave on the side algebraic structures and
% just look at modelling simple sets, some difficulties
% immediately arise.  One of them is that sets play, at least
% linguistically, two roles.  For example the set of natural numbers
% $\mathbb{N}$ could be assimilated to the type of natural numbers
% \mbox{\lstinline/nat/,} and the set of primes $\mathbb{P}$ as
% a different type.
% Types' most paradigmatic role is the one of avoiding confusion,
% and as a result two different types signal different, incompatible,
% objects.  Here we absolutely need all primes to
% be numbers too, since we want to reuse on them all the operations and
% theorems we already have on simple numbers.
% The simplest approach to this problem comes from the
% observation that $\mathbb{P}\subset\mathbb{N}$, and that $\mathbb{P}$
% can be seen as a property, assumed or proved for some elements of
% $\mathbb{N}$.  In other words model  $\mathbb{N}$ as a type and
% $\mathbb{P}$ as a predicate.  While this idea is widely used
% in the \mcbMC{} library it is not the silver bullet.
%
% As the concepts one models become more and more
% sophisticated the number of properties and operations that
% characterize them grow, and this very simple approach does not help in
% organizing and structuring such knowledge.
%
% If we follow this approach the type of prime numbers can be seen
% as a type that combines a natural number with a proof that such
% number is prime.  Hence a prime, by forgetting such primality witness,
% is a simple number and can be used as such.
%
%
% This construction, a \emph{dependent pair packing data and proofs}, is
% tricky to get right in the \mcbCIC{}, and the solution adopted in the
% \mcbMC{} library is the subject of this chapter.  In the next chapter
% we will see that such construction scales up surprisingly well, to the
% point where it is also used to model carefully structured mathematics,
% like algebra.

First of all, the \mcbCIC{} has no built-in notion of sub-typing and
what we propose ``implements'' the mechanics of sub-typing on top
of records and programmable type inference.  We begin by studying a
simple example of sub-type and reach our proposed encoding by first
showing some alternatives and discussing their drawbacks.

We pick the sub-type of sequences of a given length as our case study.
The objective is to ease the reasoning on sequences when manipulated
by operations that alters the length in a known way, while retaining
all the theory we already have on general sequences.

An approach to describe such data type that is possible in type
theories with indexed inductive families like the \mcbCIC{} is to
craft a new ad-hoc type, the type of vectors.

\begin{coq}{name=vect}{width=\textwidth,title=Vectors}
Inductive vect T : nat -> Type :=
| vnil : vect T 0
| vcons n of T & vect T n : vect T n.+1.
\end{coq}

In this construction the type carries the extra information we want.
For example \lstinline/(vect nat 7)/ describes a sequence of natural
numbers of length 7.
One can define operations on the \lstinline/vect/ data type that
signal, in their type, the effect they have on the length of the
vectors they manipulate.  For example one can read in the type of the
function that reverses a vector that it preserves the length
(\lstinline/n/ here).

\begin{coq}{name=vrev}{width=\textwidth,title=Vector reversal}
Definition vcast {T n w} (e : w = n) (v : vect T n) : vect T w :=
  let: erefl in _ = w := esym e return vect T w in v.
  Definition vrev {T n} (v : vect T n) : vect T n :=
  let fix aux m (acc : vect T m) n (v : vect T n) : vect T (m+n) :=
    if v isn't vcons n' x xs then vcast (addn0 m) acc
    else vcast (addnS m n') (aux m.+1 (vcons x acc) n' xs) in
  aux 0 (vnil T) n v.
\end{coq}

This means that there will be no need to convince Coq that
\lstinline/vrev/ preserves the length of the input vector
each time it is used: we morally prove it once an for all
when we define the operation.

\begin{coq}{name=vlen}{width=\textwidth,title=Vector assisted proof}
Definition vlength {T n} (v : vect T n) := n.
Lemma simple T n (v : vect T n) : vlength v = vlength (vrev v).
Proof. by []. Qed.
\end{coq}
\coqrun{name=vect2}{ssr,vect,vrev,vlen}

The drawback of this approach is that the type \lstinline/(vect T n)/
and \lstinline/(seq T)/ are different and we can't directly apply to a
vector an operation that is defined on sequences.  For example we
already have a reversal function for sequences called \lstinline/rev/,
but writing \lstinline/(rev vnil)/ results in a type error.  And when
new operations have to be defined, new proofs have to be done. For
example, even if we already proved that \lstinline/rev/ is an
involution, we have to prove the same property from scratch for
\lstinline/vrev/.

Of course one can relate sequences and vectors by means of
morphisms.  Even if the \mcbMC{} library provides adequate
infrastructure for dealing with morphisms, its use is generally
avoided when possible.  GIVE A BETTER INSIGHT.

The conclusion is to drop the idea of defining a completely new data
type, and try to reuse the one of sequences.  For example by packing a
sequence with a proof that witnesses that its length is exactly
\lstinline/n/.  Such construction, a dependent pair packing data
and proofs, is tricky to get right in the \mcbCIC{}.  We start with
the following ``naive'' record type \lstinline/(lseq T n)/:

\begin{coq}{name=haslen}{width=\textwidth,title=Simple dependent pair}
Inductive has_len (T : Type) : seq T -> nat -> Prop :=
| base : has_len [::] 0
| step n l a of has_len l n : has_len [:: a & l] n.+1.

Record lseq T n := Lseq { lval : seq T; llen : has_len lval n }.
\end{coq}

The advantage of this approach over the former is that the
\lstinline/lval/ projection can be used to get a \lstinline/seq/ out
of an \lstinline/lseq/ directly.  And using the \lstinline/Coercion/
mechanism the system would even insert it automatically.  Hence
writing \lstinline/(rev l)/ for \lstinline/(l : lseq T n)/ would
be accepted by the system.

Note that so far records only contained types and terms, typically
a function (like the comparison used to implement the overload
\lstinline/==/ syntax).  Here the record contains \emph{a term and a proof}.
Proofs are first class citizens in all meanings, and this may lead to
a somewhat surprising behavior of the type theory.
When we defined the function to compare pairs we had no
doubts: both components of the pair play a role and must be taken into
account.  When comparing two elements of \lstinline/lseq/ we would
like the second component not to matter.  In other words we want this
lemma to be provable:

\begin{coq}{name=lval}{width=\textwidth,title=Injectivity of lval}
Lemma lval_inj T n (l1 l2 : lseq T n) : lval l1 = lval l2 -> l1 = l2.
\end{coq}
\coqrun{name=lval}{ssr,haslen,lval,abort}

Intuitively it says that two sequence of length \lstinline/n/ are
equal if they are equal as sequences, it does not matter \emph{how one
proved} that they have length \lstinline/n/.

Unfortunately this fact is in general not true in \mcbCIC{}: all
record components are relevant to equality.
Luckily, for some inductive relations like \lstinline/has_len/, one
can show that their inhabitants (that are proofs) are canonical.  In
other words that two proofs of the same statement are equal.
Unfortunately such proof, when possible, is not only tricky, but
it also depends on the shape of the inductive relation.  Hence every
time one wants to form a sub-type expressing a new property, he has to
prove such lemma again for the new predicate.

To the rescue comes the result of Hedberg~\cite{Hedberg}
that proves such property for a wide class of predicates.
In particular he shows that any type with decidable identity
has unique identity proofs.  If we pick the concrete example
of \lstinline/bool/, then all proofs that \lstinline/(b = true)/
for a fixed \lstinline/b/ are the same.
As a direct application of this result we have that, as long as we can
express the property defining the sub-type as a boolean predicate,
we can reuse the same injectivity lemma.

\begin{coq}{name=tupx}{width=\textwidth,title=Tuple sub-type of seq}
Structure tuple_of n T :=  Tuple { tval :> seq T; _ : size tval == n }.
Notation "n .-tuple T" := (tuple_of n T) (at level 2).
\end{coq}
\coqrun{name=r1}{ssr,tupx}

Recall the hidden \lstinline/is_true/ coercion.  If we unfold
its definition we clearly see that the predicate
\lstinline/(size tval == n)/ is an equality on \lstinline/bool/.

In the \mcbMC{} library, where \emph{all predicates that can} be
expressed as a boolean function \emph{are expressed as a boolean
function}, forming sub-types is extremely easy.

% For now we state only two instances of the general result of
% Hedberg.  They are enough for the following section, were
% we see how, by programming type inference, one can take
% full advantage of the tuple subtype.
%
% \begin{coq}{title=Two instances of Hedberg's theorem}
% Theorem bool_irrelevance (x y : bool) : forall e1 e2 : x = y, e1 = e2.
% Theorem nat_irrelevance (x y : nat) : forall e1 e2 : x = y, e1 = e2.
% \end{coq}\marginnote{unkeyed...}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\mcbLEVEL{1}
\mcbREQUIRE{records + CS}
\mcbPROVIDE{working with tuples}
\mcbLEARN{CS = proof search}
\mcbsection{Working with sub-types}

For the sake of this section, we will use this, equivalent, definition
of the tuple type.  Note that we use \lstinline/=/ in place of
\lstinline/==/.

\begin{coq}{name=tup}{width=\textwidth,title=Tuple sub-type of seq}
Structure tuple_of n T := Tuple { tval : seq T; _ : size tval = n }.
Notation "n .-tuple T" := (tuple_of n T) (at level 2).
\end{coq}

% Such equality relates to natural numbers, hence the
% \lstinline/nat\_irrelevance/ lemma will come handy later on.

Now we focus on the use of the tuple type.  The key property of this
type is that it tells us the length of its elements when seen as
sequences:

\begin{coq}{name=sizetup}{}
Coercion tval : tuple_of >-> seq.
Lemma size_tuple {T n} (t : n.-tuple T) : size t = n.
Proof. by case: t. Qed.
\end{coq}
\coqrun{name=r2}{ssr,tup,sizetup}

In other words each inhabitant of the tuple type brings with it,
in the form of a proof, its length.  As test bench for the notion
of tuple we pick this simple example: a tuple is processed using
functions defined on sequences, namely \lstinline/rev/ and
\lstinline/map/.

\begin{coq}{name=p1}{}
Example seq_on_tuple n (t : n.-tuple nat) :
  size (rev [seq 2 * x | x <- rev t]) = size t.
\end{coq}

There are two ways to prove that lemma.  The first one is
to ignore the fact that \lstinline/t/ is tuple, consider it
as a regular sequence, and use only the theory of sequences.

\begin{coq}{name=p2}{}
Proof. by rewrite map_rev revK size_map. Qed.
\end{coq}
\coqrun{name=r3}{ssr,tup,sizetup,p1,p2}

Mapping a function over the reverse of a list, it equivalent to
first map the function over the list and then reverse the result
(\lstinline/map_rev/).  Then, reversing twice a list is a no-op, since
\lstinline/rev/ is an involution
(\lstinline/revK/).  Finally, mapping a function over a list does not
change its size (\lstinline/size_map/).  The sequence of rewritings
make the left hand side of the conjecture identical to the right hand
side, and we can conclude.

This simple example shows that the theory of sequences is usable
on terms of type tuple.  Still we didn't take any advantage of
the fact that  \lstinline/t/ is tuple.

The second way to prove this theorem is to rely on the rich type
of \lstinline/t/ to actually compute the length of the sequence.

\begin{coq}{name=p3}{}
Example just_tuple_attempt n (t : n.-tuple nat) :
  size (rev [seq 2 * x | x <- rev t]) = size t.
Proof. rewrite size_tuple.
\end{coq}
\coqrun{name=r4;fail}{ssr,tup,sizetup,p3,show1}
\begin{coqout}{run=r4;out=g1}{}
1 subgoal

 n : nat
 t : n .-tuple nat
 ============================
 size (rev [seq 2 * x | x <- rev t]) = n
\end{coqout}

The rewriting replaces the right hand side with \lstinline/n/ as
expected, but we can't go any further: the lemma is not replacing
the right hand side with \lstinline/n/, even if we are working
with a tuple \lstinline/t/.  Why is that?  In the left hand side
\lstinline/t/ is processed using functions on sequences.
The type of \lstinline/rev/ for example is
\lstinline/(forall T, seq T -> seq T)/.  The coercion \lstinline/tval/
from \lstinline/tuple_of/ to \lstinline/seq/ make the
expression \lstinline/(rev (tval t))/ well typed, but the output
is of type \lstinline/(seq nat)/.  If we go back to our example
of the \lstinline/move/ function for points in the plane, it is like
assigning to it the type \lstinline/($\alpha$ -> P)/ for $\alpha$
being at least a point.  We would like the function to return
a data as rich as the one it takes in input.

What happens if try to unify the left hand side of the
\lstinline/size_tuple/ equation with the redex
\lstinline/(rev t)/ (and we print coercions)?

\begin{coq}{name=infrastructure}{title=Toolkit not belonging here}
(* toolkit *)
Notation "X (*...*)" := (let x := X in let y := _ in x)
  (at level 100, format "X  (*...*)").
Notation "[LHS 'of' equation ]" :=
  (let LHS := _ in
   let _infer_LHS := equation : LHS = _ in LHS)
  (at level 4).
Notation "[unify X 'with' Y ]" :=
  (let unification := erefl _ : X = Y in
   True).
\end{coq}

\begin{coq}{name=showunif}{title=Simulate rewrite size\_tuple}
Check forall T n (t : n.-tuple T),
 let LHS := [LHS of size_tuple _] in
 let RDX := size (rev t) in
 [unify LHS with RDX].
\end{coq}
\coqrun{name=r5;fail}{ssr,tup,sizetup,infrastructure,showunif}
\begin{coqout}{run=r5}{title=Response}
Error:
In environment
T : Type
n : nat
t : n .-tuple T
LHS := size (tval ?94 ?92 ?96) (*...*) : nat
RDX := size (rev (tval n T t))           : nat
The term "erefl ?95" has type "?95 = ?95" while
it is expected to have type "LHS = RDX".
\end{coqout}

Unifying \lstinline/(size (tval ?$_n$ ?$_T$ ?$_t$))/
with \lstinline/(size (rev (tval n T t)))/ is hard.
Both term's head symbol is \lstinline/size/, but then
the projection \lstinline/tval/ applied to unification
variables has to unified with \lstinline/(rev ...)/,
and both terms are in normal form.

Such problem is nontrivial because to solve it one has to infer a
record for \lstinline/?$_t$/ that contains a proof: a
tuple whose \lstinline/tval/ field
is \lstinline/rev t/ (and whose other field contains a
proof that such sequence has length \lstinline/?$_n$/).

This unification problem falls in the class handled by
canonical Canonical Structures: a record projection
against a value.  We can declare Canonical Structures
teaching Coq the effect of list operations over the length
of their input.

\begin{coq}{name=t1}{}
Lemma rev_tupleP n A (t : n.-tuple A) : size (rev t) = n.
Proof. by rewrite size_rev size_tuple. Qed.
Canonical rev_tuple n A (t : n.-tuple A) := Tuple (rev_tupleP t).

Lemma map_tupleP n A B (f: A -> B) (t : n.-tuple A) : size (map f t) = n.
Proof. by rewrite size_map size_tuple. Qed.
Canonical map_tuple n A B f (t : n.-tuple A) : n.-tuple B :=
  Tuple (map_tupleP f t).
\end{coq}

Even if it is not needed for the lemma we took as our test bench,
we add another example where the length is not preserved.

\begin{coq}{name=t2}{}
Lemma cons_tupleP n A (t : n.-tuple A) x : size (x :: t) = n.+1.
Proof. by rewrite /= size_tuple. Qed.
Canonical cons_tuple n A x (t : n.-tuple A) : n.+1 .-tuple A :=
  Tuple (cons_tupleP t x).
\end{coq}

The global table of canonical solutions is extended as follows.

\noindent
\begin{tcolorbox}[colframe=blue!60!white,before=\hfill,after=\hfill,center title,tabularx={ll|l|l},fonttitle=\sffamily\bfseries,title=Canonical Structures Index]
projection & value & solution & combines solutions for \\ \hline
\lstinline/tval N A/ & \lstinline/rev A S/ & \lstinline/rev_tuple N A T/
	& \lstinline/T/ $\leftarrow$ (\lstinline/tval N A/, \lstinline/S/) \\
\lstinline/tval N B/ & \lstinline/map A B F S/ & \lstinline/map_tuple N A B F T/
	& \lstinline/T/ $\leftarrow$ (\lstinline/tval N A/, \lstinline/S/) \\
\lstinline/tval N.+1 A/ & \lstinline/X :: S/ & \lstinline/cons_tuple N A T X/
	& \lstinline/T/ $\leftarrow$ (\lstinline/tval N A/, \lstinline/S/) \\
\end{tcolorbox}

Thanks to the now extended capability of type inference
we can prove our lemma by just reasoning about tuples.

\begin{coq}{name=t3}{}
Example just_tuple n (t : n.-tuple nat) :
  size (rev [seq 2 * x | x <- rev t]) = size t.
Proof. by rewrite !size_tuple. Qed.
\end{coq}
\coqrun{name=r6}{ssr,tup,sizetup,t1,t2,t3}

The iterated rewriting acts now twice replacing both the left hand
and the right hand side with \lstinline/n/.  It is worth observing
that the size of this proof (two rewrite steps) does not depend on the
complexity of the formula involved, while the one using only the
theory of lists requires one step per list-manipulating function.
What depends on the size of the formula is the number of canonical
structure resolution steps type inference performs.  Another advantage
of this last approach is that, unlike in the first, one
is not required to know the names of the lemmas:
it the new concept of tuple that takes care of the size related
reasoning.

Going back to the example of (coloured) points, once a coloured point
is defined as a record embedding a simple point

\begin{coq}{name=col}{}
Record CP := Coloured { coords_of :> P; colour_of : color }
\end{coq}

\noindent
the way we implement sub-type polymorphism between \lstinline/CP/
and \lstinline/P/ is by assigning the typed \lstinline/(P -> P)/ to
\lstinline/move/ but:
\begin{itemize}
\item thanks to the automatically inserted
	\emph{forgetful coercion} \lstinline/coords_of/
	we can apply \lstinline/move/ to a coloured point \lstinline/c/
	obtaining \lstinline/(move (coords_of c))/
\item thanks to \emph{programmable type inference} such	expression
	can be automatically injected back in the type of coloured
	points by using the color of \lstinline/c/ and obtain
	\lstinline/(Coloured (move (coords_of c))$~$(colour_of c))/
\end{itemize}
That is pretty much what ones expects \lstinline/move/ to do when
applied to a coloured point: act on its coordinates and leave
its colour untouched.

We have seen how to implement the glue that links a type (sequences)
with a sub-type (tuples), but only in a very simple case.
A tuple is trivially a sequence (by forgetting something), hence the
theory of sequences ``trivially'' applies to tuples.

We now revise the definition of \lstinline/eqType/, turning it into a
real interface bringing with it a specific theory.  Later we show that
since \lstinline/seq/ is an instance of that interfaces and since
\lstinline/tuple/ is a sub-type of \lstinline/seq/, then the theory
of \lstinline/eqType/ also applies to tuples.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\mcbLEVEL{1}
\mcbREQUIRE{proof language}
\mcbPROVIDE{an example of sub-type of eqType}
\mcbLEARN{it is a schematic process}
\mcbsection{Building the \lstinline/eqType/ for \lstinline/n.-tuples T/}

We have that \lstinline/(seq T)/ is an \lstinline/eqType/ and
we want to transport \lstinline/==/ on tuples.  First we need to
defined a comparison function for tuples.

\begin{coq}{name=tupcmp}{title=Comparison of tuples}
Definition tcmp n (T : eqType) (t1 t2 : n.-tuple T) :=
  tval t1 == tval t2.
\end{coq}
\coqrun{name=tupcmp}{ssr,require-tuple,tupcmp}

Here we reuse the one on sequences, and we ignore the
proof part of tuples.

We need now to prove

\begin{coqdef}{name=eqtupP}
Lemma eqtupleP n (T : eqType) : Equality.axiom (@tcmp n T).
Proof.
move=> x y; apply: (iffP eqP); last first.
Redirect "g1" Show.
  by move->.
case: x; case: y => s1 p1 s2 p2 /= E. Redirect "g2" Show.
rewrite E in p2 *.
by rewrite (eq_irrelevance p1 p2).
Qed.
\end{coqdef}
\begin{coq}{def=eqtupP}{}
Lemma eqtupleP n (T : eqType) : Equality.axiom (@tcmp n T).
Proof.
move=> x y; apply: (iffP eqP); last first.
  by move->.
case: x; case: y => s1 p1 s2 p2 /= E.
rewrite E in p2 *.
by rewrite (eq_irrelevance p1 p2).
Qed.
\end{coq}
\coqrun{name=tupcmpP}{ssr,tup,tupcmp,eqtupP}

The first direction is trivial

\begin{coqout}{run=tupcmpP;out=g1}{title=Response line 4,width=5cm}
2 focused subgoals (shelved: 2)

n : nat
T : eqType
x, y : n .-tuple T
============================
x = y -> tval x = tval y

subgoal 2 is:
  tval x = tval y -> x = y
\end{coqout}

For the other one the crucial step is the use
of \lstinline/eq_irrelevance/.

\begin{coqout}{run=tupcmpP;out=g2}{title=Response line 6,width=7cm}
1 subgoal

n : nat
T : eqType
s1 : seq T
p1 : size s1 = n
s2 : seq T
p2 : size s2 = n
E : s2 = s1
============================
Tuple p2 = Tuple p1
\end{coqout}

We can then declare

\begin{coq}{name=canontup}{}
Canonical tuple_eqType n T :=
  Equality.Pack (Equality.Mixin (@eqtupleP n T)).
\end{coq}

Simple test

\begin{coq}{name=testtup}{}
Check forall (t : 3.-tuple nat), [:: t] == [::].
Check fun t : 3.-tuple nat => uniq [:: t; t].
Check fun t : 3.-tuple nat => undup_uniq [:: t; t].
\end{coq}
\coqrun{name=tupc}{ssr,require-tuple,testtup}

We did it by hand, but all that is schematic, any sigma type can be
dealt with in the very same way.  When one has a base type T and a
sub-type ST defined as a boolean sigma type, then one can build all
the canonical instances by just knowing the name of the projection
going from ST to T.

The lirbary is equipped with all the canonical structures that
possibly apply.  Only the reader willing to extend the library with
new concepts is interested by what follows.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\mcbLEVEL{2}
\mcbLEARN{Declare a new subtype and derive eqType}
\mcbPROVIDE{usage of \lstinline/[subType for ...]/}
\mcbREQUIRE{}
\mcbsection{The toolkit to declare a new sub-type}

As simple as such

\begin{coq}{name=subt}{}
Canonical tuple_subType := Eval hnf in [subType for tval].
Definition tuple_eqMixin := Eval hnf in [eqMixin of n.-tuple T by <:].
Canonical tuple_eqType := Eval hnf in EqType (n.-tuple T) tuple_eqMixin.
\end{coq}

maybe we can put here the nice trick for encoding a bizarre structure
into a three and get the eqType out of it.  In such case we need to
rename the section into: declaring a new eqType (not only for
subtypes).

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\mcbLEVEL{3}
\mcbLEARN{How sub-type works}
\mcbPROVIDE{super CS skills}
\mcbREQUIRE{CS}
\mcbsection{The \lstinline/subType/ infrastructure}

\begin{coq}{name=subtdef}{}
Structure subType : Type := SubType {
  sub_sort :> Type;
  val : sub_sort -> T;
  Sub : forall x, P x -> sub_sort;
  (* elim rule for the record *)
  _ : forall K (_ : forall x Px, K (@Sub x Px)) u, K u;
  _ : forall x Px, val (@Sub x Px) = x
}.

Notation "[ 'subType' 'for' v ]" := (SubType _ v _ inlined_sub_rect vrefl_rect)
 (at level 0, only parsing) : form_scope.
\end{coq}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\mcbLEVEL{2}
\mcbLEARN{curiosity}
\mcbPROVIDE{}
\mcbREQUIRE{}
\mcbsection{Sub-types in HoTT}

Recent advances in HoTT identify the class of propositions that have a
canonical proof, mere propositions, and such class is closed under
forall quantification, while the one we use is not (only bounded
forall) but the idea is pretty much the same.  Mere propositions can
be easily used to form a subtype.

Also Cyril had other arguments on the fact that one can
ask less (be more general) and recover all required properties later
on, but I've to ask him again cause I've forgotten the example.
